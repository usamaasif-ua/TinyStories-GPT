# Small Language Model from Scratch

[![Hugging Face Model](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Model-yellow)](https://huggingface.co/usamaasif-ua/GPT-TinyStories-512)

A GPT-style language model built from scratch using PyTorch. This project demonstrates how to implement a transformer-based language model with approximately 51 million parameters, trained on the TinyStories dataset.

> **ðŸ“¦ Pretrained Model:** Download the trained model weights from [Hugging Face](https://huggingface.co/usamaasif-ua/GPT-TinyStories-512)

## Overview

This repository contains a complete implementation of a decoder-only transformer language model. The model is designed to generate creative and coherent short stories similar to those a young child might understand.

### Model Architecture

| Component | Specification |
|-----------|---------------|
| Parameters | ~51 million |
| Layers | 8 transformer blocks |
| Attention Heads | 8 |
| Embedding Dimension | 512 |
| Context Window | 256 tokens |
| Vocabulary Size | 50,257 (GPT-2 tokenizer) |

### Key Features

- Custom implementation of multi-head causal self-attention with Flash Attention support
- Pre-norm transformer architecture with LayerNorm
- Weight tying between token embeddings and output layer
- Mixed precision training with automatic gradient scaling
- Cosine annealing learning rate schedule with linear warmup

## Dataset

The model is trained on [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories), a synthetic dataset of short stories generated by GPT-3.5 and GPT-4. The stories use vocabulary and concepts that a typical 3 to 4 year old would understand.

## Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA compatible GPU (recommended)

### Dependencies

```
torch
numpy
tiktoken
datasets
tqdm
```

## Usage

### Training

Open the Jupyter notebook and run all cells sequentially:

```
Small_Language_Model_Scratch_Final.ipynb
```

The training configuration uses the following hyperparameters:

| Parameter | Value |
|-----------|-------|
| Learning Rate | 5e-4 |
| Batch Size | 32 |
| Gradient Accumulation Steps | 32 |
| Max Iterations | 40,000 |
| Warmup Steps | 2,000 |

Training takes approximately 9 hours on a modern GPU.

### Generating Text

After training, you can generate text using the model:

```python
import tiktoken

enc = tiktoken.get_encoding("gpt2")
sentence = "Once upon a time there was a pumpkin."
context = torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim=0)
output = model.generate(context, max_new_tokens=200)
print(enc.decode(output.squeeze().tolist()))
```

### Loading Pretrained Weights

To load the trained model:

```python
model = GPT(config)
model.load_state_dict(torch.load("best_model_params.pt"))
model.eval()
```

## Project Structure

```
SLM/
â”œâ”€â”€ Small_Language_Model_Scratch_Final.ipynb   # Main notebook with implementation
â”œâ”€â”€ best_model_params.pt                        # Trained model weights
â”œâ”€â”€ train.bin                                   # Tokenized training data
â”œâ”€â”€ validation.bin                              # Tokenized validation data
â””â”€â”€ README.md
```

## Training Progress

The model achieves the following loss values during training:

| Iteration | Train Loss | Validation Loss |
|-----------|------------|-----------------|
| 1,000 | 5.46 | 5.46 |
| 5,000 | 3.07 | 3.07 |
| 10,000 | 2.38 | 2.39 |
| 20,000 | 1.89 | 1.92 |
| 40,000 | 1.51 | 1.57 |

## References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT-2 Paper)
- [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759) (Eldan and Li, 2023)
- [nanoGPT](https://github.com/karpathy/nanoGPT) by Andrej Karpathy

## License

This project is available for educational and research purposes.
